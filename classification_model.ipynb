{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Katya -  Classification Model",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AXKNifvB_-m"
      },
      "source": [
        "### 0) Setting up environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbgOCkrvtMcr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc4ecbc7-ca63-4795-9fbd-6001a1d74a03"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXRaJxQTtmfO"
      },
      "source": [
        "!pip install fasttext\n",
        "!pip install nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBwfHX3Gtbry"
      },
      "source": [
        "### 1) Getting the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKo4U3jxcK-O",
        "outputId": "7347a25a-a95d-42dd-f5e4-7c172f69cf00"
      },
      "source": [
        "import fasttext\n",
        "from sklearn.model_selection import train_test_split\n",
        "import nltk\n",
        "import csv\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKcjlqX9tR3N",
        "outputId": "f06a76bc-00ee-41ac-8553-050712b3f2e6"
      },
      "source": [
        "\n",
        "\n",
        "# labels for all languages\n",
        "labels_filename = '/content/gdrive/My Drive/wili-2018/labels.csv'\n",
        "\n",
        "with open(labels_filename,'r') as data: \n",
        "  reader = csv.reader(data)\n",
        "  keys = ((next(reader))[0]).split(';')  \n",
        "\n",
        "  labels_dict = {}\n",
        "  for line in reader: \n",
        "\n",
        "    # 'Label', 'English', 'Wiki Code', 'ISO 369-3', 'German', 'Language family', 'Writing system', 'Remarks', 'Synonyms'\n",
        "    label, lang, wiki_code, _, _, lang_family, _, _, _ = (line[0]).split(';') \n",
        "\n",
        "    lang_dict = {'lang': lang, 'wiki_code': wiki_code, 'lang_family': lang_family}\n",
        "    labels_dict[label] = lang_dict\n",
        "\n",
        "print(\"label dictionary: \", labels_dict)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "label dictionary:  {'ace': {'lang': 'Achinese', 'wiki_code': 'ace', 'lang_family': 'Austronesian'}, 'afr': {'lang': 'Afrikaans', 'wiki_code': 'af', 'lang_family': 'Indo-European'}, 'als': {'lang': 'Alemannic German', 'wiki_code': 'als', 'lang_family': 'Indo-European'}, 'amh': {'lang': 'Amharic', 'wiki_code': 'am', 'lang_family': 'Afro-Asiatic'}, 'ang': {'lang': 'Old English ', 'wiki_code': 'ang', 'lang_family': 'Indo-European'}, 'ara': {'lang': 'Arabic', 'wiki_code': 'ar', 'lang_family': 'Afro-Asiatic'}, 'arg': {'lang': 'Aragonese', 'wiki_code': 'an', 'lang_family': 'Indo-European'}, 'arz': {'lang': 'Egyptian Arabic', 'wiki_code': 'arz', 'lang_family': 'Afro-Asiatic'}, 'asm': {'lang': 'Assamese', 'wiki_code': 'as', 'lang_family': 'Indo-European'}, 'ast': {'lang': 'Asturian', 'wiki_code': 'ast', 'lang_family': 'Indo-European'}, 'ava': {'lang': 'Avar', 'wiki_code': 'av', 'lang_family': 'Northeast Caucasian'}, 'aym': {'lang': 'Aymara', 'wiki_code': 'ay', 'lang_family': 'Aymaran'}, 'azb': {'lang': 'South Azerbaijani', 'wiki_code': 'azb', 'lang_family': 'Turkic'}, 'aze': {'lang': 'Azerbaijani', 'wiki_code': 'az', 'lang_family': 'Turkic'}, 'bak': {'lang': 'Bashkir', 'wiki_code': 'ba', 'lang_family': 'Turkic'}, 'bar': {'lang': 'Bavarian', 'wiki_code': 'bar', 'lang_family': 'Indo-European'}, 'bcl': {'lang': 'Central Bikol', 'wiki_code': 'bcl', 'lang_family': 'Austronesian'}, 'be-tarask': {'lang': 'Belarusian (Taraschkewiza)', 'wiki_code': 'be-tarask', 'lang_family': 'Indo-European'}, 'bel': {'lang': 'Belarusian', 'wiki_code': 'be', 'lang_family': 'Indo-European'}, 'ben': {'lang': 'Bengali', 'wiki_code': 'bn', 'lang_family': 'Indo-European'}, 'bho': {'lang': 'Bhojpuri', 'wiki_code': 'bh', 'lang_family': 'Indo-European'}, 'bjn': {'lang': 'Banjar', 'wiki_code': 'bjn', 'lang_family': 'Austronesian'}, 'bod': {'lang': 'Tibetan', 'wiki_code': 'bo', 'lang_family': 'Sino-Tibetan'}, 'bos': {'lang': 'Bosnian', 'wiki_code': 'bs', 'lang_family': 'Indo-European'}, 'bpy': {'lang': 'Bishnupriya', 'wiki_code': 'bpy', 'lang_family': 'Indo-European'}, 'bre': {'lang': 'Breton', 'wiki_code': 'br', 'lang_family': 'Indo-European'}, 'bul': {'lang': 'Bulgarian', 'wiki_code': 'bg', 'lang_family': 'Indo-European'}, 'bxr': {'lang': 'Buryat', 'wiki_code': 'bxr', 'lang_family': 'Mongolic'}, 'cat': {'lang': 'Catalan', 'wiki_code': 'ca', 'lang_family': 'Indo-European'}, 'cbk': {'lang': 'Chavacano', 'wiki_code': 'cbk-zam', 'lang_family': 'Indo-European'}, 'cdo': {'lang': 'Min Dong', 'wiki_code': 'cdo', 'lang_family': 'Sino-Tibetan'}, 'ceb': {'lang': 'Cebuano', 'wiki_code': 'ceb', 'lang_family': 'Austronesian'}, 'ces': {'lang': 'Czech', 'wiki_code': 'cs', 'lang_family': 'Indo-European'}, 'che': {'lang': 'Chechen', 'wiki_code': 'ce', 'lang_family': 'Northeast Caucasian'}, 'chr': {'lang': 'Cherokee', 'wiki_code': 'chr', 'lang_family': 'Iroquoian'}, 'chv': {'lang': 'Chuvash', 'wiki_code': 'cv', 'lang_family': 'Turkic'}, 'ckb': {'lang': 'Central Kurdish', 'wiki_code': 'ckb', 'lang_family': 'Indo-European'}, 'cor': {'lang': 'Cornish', 'wiki_code': 'kw', 'lang_family': 'Indo-European'}, 'cos': {'lang': 'Corsican', 'wiki_code': 'co', 'lang_family': 'Indo-European'}, 'crh': {'lang': 'Crimean Tatar', 'wiki_code': 'crh', 'lang_family': 'Turkic'}, 'csb': {'lang': 'Kashubian', 'wiki_code': 'csb', 'lang_family': 'Indo-European'}, 'cym': {'lang': 'Welsh', 'wiki_code': 'cy', 'lang_family': 'Indo-European'}, 'dan': {'lang': 'Danish', 'wiki_code': 'da', 'lang_family': 'Indo-European'}, 'deu': {'lang': 'German', 'wiki_code': 'de', 'lang_family': 'Indo-European'}, 'diq': {'lang': 'Dimli', 'wiki_code': 'diq', 'lang_family': 'Indo-European'}, 'div': {'lang': 'Dhivehi', 'wiki_code': 'dv', 'lang_family': 'Indo-European'}, 'dsb': {'lang': 'Lower Sorbian', 'wiki_code': 'dsb', 'lang_family': 'Indo-European'}, 'dty': {'lang': 'Doteli', 'wiki_code': 'dty', 'lang_family': 'Indo-European'}, 'egl': {'lang': 'Emilian', 'wiki_code': 'eml', 'lang_family': 'Indo-European'}, 'ell': {'lang': 'Modern Greek', 'wiki_code': 'el', 'lang_family': 'Indo-European'}, 'eng': {'lang': 'English', 'wiki_code': 'en', 'lang_family': 'Indo-European'}, 'epo': {'lang': 'Esperanto', 'wiki_code': 'eo', 'lang_family': 'Constructed'}, 'est': {'lang': 'Estonian', 'wiki_code': 'et', 'lang_family': 'Uralic'}, 'eus': {'lang': 'Basque', 'wiki_code': 'eu', 'lang_family': 'Language isolate'}, 'ext': {'lang': 'Extremaduran', 'wiki_code': 'ext', 'lang_family': 'Indo-European'}, 'fao': {'lang': 'Faroese', 'wiki_code': 'fo', 'lang_family': 'Indo-European'}, 'fas': {'lang': 'Persian', 'wiki_code': 'fa', 'lang_family': 'Indo-European'}, 'fin': {'lang': 'Finnish', 'wiki_code': 'fi', 'lang_family': 'Uralic'}, 'fra': {'lang': 'French', 'wiki_code': 'fr', 'lang_family': 'Indo-European'}, 'frp': {'lang': 'Arpitan', 'wiki_code': 'frp', 'lang_family': 'Indo-European'}, 'fry': {'lang': 'Western Frisian', 'wiki_code': 'fy', 'lang_family': 'Indo-European'}, 'fur': {'lang': 'Friulian', 'wiki_code': 'fur', 'lang_family': 'Indo-European'}, 'gag': {'lang': 'Gagauz', 'wiki_code': 'gag', 'lang_family': 'Turkic'}, 'gla': {'lang': 'Scottish Gaelic', 'wiki_code': 'gd', 'lang_family': 'Indo-European'}, 'gle': {'lang': 'Irish', 'wiki_code': 'ga', 'lang_family': 'Indo-European'}, 'glg': {'lang': 'Galician', 'wiki_code': 'gl', 'lang_family': 'Indo-European'}, 'glk': {'lang': 'Gilaki', 'wiki_code': 'glk', 'lang_family': 'Indo-European'}, 'glv': {'lang': 'Manx', 'wiki_code': 'gv', 'lang_family': 'Indo-European'}, 'grn': {'lang': 'Guarani', 'wiki_code': 'gn', 'lang_family': 'Tupi-Guarani'}, 'guj': {'lang': 'Gujarati', 'wiki_code': 'gu', 'lang_family': 'Indo-European'}, 'hak': {'lang': 'Hakka Chinese', 'wiki_code': 'hak', 'lang_family': 'Sino-Tibetan'}, 'hat': {'lang': 'Haitian Creole', 'wiki_code': 'ht', 'lang_family': 'Indo-European'}, 'hau': {'lang': 'Hausa', 'wiki_code': 'ha', 'lang_family': 'Afro-Asiatic'}, 'hbs': {'lang': 'Serbo-Croatian', 'wiki_code': 'sh', 'lang_family': 'Indo-European'}, 'heb': {'lang': 'Hebrew', 'wiki_code': 'he', 'lang_family': 'Afro-Asiatic'}, 'hif': {'lang': 'Fiji Hindi', 'wiki_code': 'hif', 'lang_family': 'Indo-European'}, 'hin': {'lang': 'Hindi', 'wiki_code': 'hi', 'lang_family': 'Indo-European'}, 'hrv': {'lang': 'Croatian', 'wiki_code': 'hr', 'lang_family': 'Indo-European'}, 'hsb': {'lang': 'Upper Sorbian', 'wiki_code': 'hsb', 'lang_family': 'Indo-European'}, 'hun': {'lang': 'Hungarian', 'wiki_code': 'hu', 'lang_family': 'Uralic'}, 'hye': {'lang': 'Armenian', 'wiki_code': 'hy', 'lang_family': 'Indo-European'}, 'ibo': {'lang': 'Igbo', 'wiki_code': 'ig', 'lang_family': 'Niger-Congo'}, 'ido': {'lang': 'Ido', 'wiki_code': 'io', 'lang_family': 'Constructed'}, 'ile': {'lang': 'Interlingue', 'wiki_code': 'ie', 'lang_family': 'Constructed'}, 'ilo': {'lang': 'Iloko', 'wiki_code': 'ilo', 'lang_family': 'Austronesian'}, 'ina': {'lang': 'Interlingua', 'wiki_code': 'ia', 'lang_family': 'Constructed'}, 'ind': {'lang': 'Indonesian', 'wiki_code': 'id', 'lang_family': 'Austronesian'}, 'isl': {'lang': 'Icelandic', 'wiki_code': 'is', 'lang_family': 'Indo-European'}, 'ita': {'lang': 'Italian', 'wiki_code': 'it', 'lang_family': 'Indo-European'}, 'jam': {'lang': 'Jamaican Patois', 'wiki_code': 'jam', 'lang_family': 'Indo-European'}, 'jav': {'lang': 'Javanese', 'wiki_code': 'jv', 'lang_family': 'Austronesian'}, 'jbo': {'lang': 'Lojban', 'wiki_code': 'jbo', 'lang_family': 'Constructed'}, 'jpn': {'lang': 'Japanese', 'wiki_code': 'ja', 'lang_family': 'Japonic'}, 'kaa': {'lang': 'Karakalpak', 'wiki_code': 'kaa', 'lang_family': 'Turkic'}, 'kab': {'lang': 'Kabyle', 'wiki_code': 'kab', 'lang_family': 'Afro-Asiatic'}, 'kan': {'lang': 'Kannada', 'wiki_code': 'kn', 'lang_family': 'Dravidian'}, 'kat': {'lang': 'Georgian', 'wiki_code': 'ka', 'lang_family': 'South Caucasian'}, 'kaz': {'lang': 'Kazakh', 'wiki_code': 'kk', 'lang_family': 'Turkic'}, 'kbd': {'lang': 'Kabardian', 'wiki_code': 'kbd', 'lang_family': 'Northeast Caucasian'}, 'khm': {'lang': 'Central Khmer', 'wiki_code': 'km', 'lang_family': 'Austronesian'}, 'kin': {'lang': 'Kinyarwanda', 'wiki_code': 'rw', 'lang_family': 'Niger-Congo'}, 'kir': {'lang': 'Kirghiz', 'wiki_code': 'ky', 'lang_family': 'Turkic'}, 'koi': {'lang': 'Komi-Permyak', 'wiki_code': 'koi', 'lang_family': 'Uralic'}, 'kok': {'lang': 'Konkani', 'wiki_code': 'gom', 'lang_family': 'Indo-European'}, 'kom': {'lang': 'Komi', 'wiki_code': 'kv', 'lang_family': 'Uralic'}, 'kor': {'lang': 'Korean', 'wiki_code': 'ko', 'lang_family': 'Koreanic'}, 'krc': {'lang': 'Karachay-Balkar', 'wiki_code': 'krc', 'lang_family': 'Turkic'}, 'ksh': {'lang': 'Ripuarisch', 'wiki_code': 'ksh', 'lang_family': 'Indo-European'}, 'kur': {'lang': 'Kurdish', 'wiki_code': 'ku', 'lang_family': 'Indo-European'}, 'lad': {'lang': 'Ladino', 'wiki_code': 'lad', 'lang_family': 'Indo-European'}, 'lao': {'lang': 'Lao', 'wiki_code': 'lo', 'lang_family': 'Tai-Kadai'}, 'lat': {'lang': 'Latin', 'wiki_code': 'la', 'lang_family': 'Indo-European'}, 'lav': {'lang': 'Latvian', 'wiki_code': 'lv', 'lang_family': 'Indo-European'}, 'lez': {'lang': 'Lezghian', 'wiki_code': 'lez', 'lang_family': 'Northeast Caucasian'}, 'lij': {'lang': 'Ligurian', 'wiki_code': 'lij', 'lang_family': 'Indo-European'}, 'lim': {'lang': 'Limburgan', 'wiki_code': 'li', 'lang_family': 'Indo-European'}, 'lin': {'lang': 'Lingala', 'wiki_code': 'ln', 'lang_family': 'Niger-Congo'}, 'lit': {'lang': 'Lithuanian', 'wiki_code': 'lt', 'lang_family': 'Indo-European'}, 'lmo': {'lang': 'Lombard', 'wiki_code': 'lmo', 'lang_family': 'Indo-European'}, 'lrc': {'lang': 'Northern Luri', 'wiki_code': 'lrc', 'lang_family': 'Indo-European'}, 'ltg': {'lang': 'Latgalian', 'wiki_code': 'ltg', 'lang_family': 'Indo-European'}, 'ltz': {'lang': 'Luxembourgish', 'wiki_code': 'lb', 'lang_family': 'Indo-European'}, 'lug': {'lang': 'Luganda', 'wiki_code': 'lg', 'lang_family': 'Niger-Congo'}, 'lzh': {'lang': 'Literary Chinese', 'wiki_code': 'zh-classical', 'lang_family': 'Sino-Tibetan'}, 'mai': {'lang': 'Maithili', 'wiki_code': 'mai', 'lang_family': 'Indo-European'}, 'mal': {'lang': 'Malayalam', 'wiki_code': 'ml', 'lang_family': 'Dravidian'}, 'map-bms': {'lang': 'Banyumasan', 'wiki_code': 'map-bms', 'lang_family': 'Austronesian'}, 'mar': {'lang': 'Marathi', 'wiki_code': 'mr', 'lang_family': 'Indo-European'}, 'mdf': {'lang': 'Moksha', 'wiki_code': 'mdf', 'lang_family': 'Uralic'}, 'mhr': {'lang': 'Eastern Mari', 'wiki_code': 'mhr', 'lang_family': 'Uralic'}, 'min': {'lang': 'Minangkabau', 'wiki_code': 'min', 'lang_family': 'Austronesian'}, 'mkd': {'lang': 'Macedonian', 'wiki_code': 'mk', 'lang_family': 'Indo-European'}, 'mlg': {'lang': 'Malagasy', 'wiki_code': 'mg', 'lang_family': 'Austronesian'}, 'mlt': {'lang': 'Maltese', 'wiki_code': 'mt', 'lang_family': 'Afro-Asiatic'}, 'mon': {'lang': 'Mongolian', 'wiki_code': 'mn', 'lang_family': 'Mongolic'}, 'mri': {'lang': 'Maori', 'wiki_code': 'mi', 'lang_family': 'Austronesian'}, 'mrj': {'lang': 'Western Mari', 'wiki_code': 'mrj', 'lang_family': 'Uralic'}, 'msa': {'lang': 'Malay', 'wiki_code': 'ms', 'lang_family': 'Austronesian'}, 'mwl': {'lang': 'Mirandese', 'wiki_code': 'mwl', 'lang_family': 'Indo-European'}, 'mya': {'lang': 'Burmese', 'wiki_code': 'my', 'lang_family': 'Sino-Tibetan'}, 'myv': {'lang': 'Erzya', 'wiki_code': 'myv', 'lang_family': 'Uralic'}, 'mzn': {'lang': 'Mazanderani', 'wiki_code': 'mzn', 'lang_family': 'Indo-European'}, 'nan': {'lang': 'Min Nan Chinese', 'wiki_code': 'zh-min-nan', 'lang_family': 'Sino-Tibetan'}, 'nap': {'lang': 'Neapolitan', 'wiki_code': 'nap', 'lang_family': 'Indo-European'}, 'nav': {'lang': 'Navajo', 'wiki_code': 'nv', 'lang_family': 'Dené-Yeniseian'}, 'nci': {'lang': 'Classical Nahuatl', 'wiki_code': 'nah', 'lang_family': 'Uto-Aztecan'}, 'nds': {'lang': 'Low German', 'wiki_code': 'nds', 'lang_family': 'Indo-European'}, 'nds-nl': {'lang': 'West Low German', 'wiki_code': 'nds-nl', 'lang_family': 'Indo-European'}, 'nep': {'lang': 'Nepali (macrolanguage)', 'wiki_code': 'ne', 'lang_family': 'Indo-European'}, 'new': {'lang': 'Newari', 'wiki_code': 'new', 'lang_family': 'Sino-Tibetan'}, 'nld': {'lang': 'Dutch', 'wiki_code': 'nl', 'lang_family': 'Indo-European'}, 'nno': {'lang': 'Norwegian Nynorsk', 'wiki_code': 'nn', 'lang_family': 'Indo-European'}, 'nob': {'lang': 'Bokmål', 'wiki_code': 'no', 'lang_family': 'Indo-European'}, 'nrm': {'lang': 'Narom', 'wiki_code': 'nrm', 'lang_family': 'Austronesian'}, 'nso': {'lang': 'Northern Sotho', 'wiki_code': 'nso', 'lang_family': 'Niger-Congo'}, 'oci': {'lang': 'Occitan', 'wiki_code': 'oc', 'lang_family': 'Indo-European'}, 'olo': {'lang': 'Livvi-Karelian', 'wiki_code': 'olo', 'lang_family': 'Uralic'}, 'ori': {'lang': 'Oriya', 'wiki_code': 'or', 'lang_family': 'Indo-European'}, 'orm': {'lang': 'Oromo', 'wiki_code': 'om', 'lang_family': 'Afro-Asiatic'}, 'oss': {'lang': 'Ossetian', 'wiki_code': 'os', 'lang_family': 'Indo-European'}, 'pag': {'lang': 'Pangasinan', 'wiki_code': 'pag', 'lang_family': 'Austronesian'}, 'pam': {'lang': 'Pampanga', 'wiki_code': 'pam', 'lang_family': 'Austronesian'}, 'pan': {'lang': 'Panjabi', 'wiki_code': 'pa', 'lang_family': 'Indo-European'}, 'pap': {'lang': 'Papiamento', 'wiki_code': 'pap', 'lang_family': 'Indo-European'}, 'pcd': {'lang': 'Picard', 'wiki_code': 'pcd', 'lang_family': 'Indo-European'}, 'pdc': {'lang': 'Pennsylvania German', 'wiki_code': 'pdc', 'lang_family': 'Indo-European'}, 'pfl': {'lang': 'Palatine German', 'wiki_code': 'pfl', 'lang_family': 'Indo-European'}, 'pnb': {'lang': 'Western Panjabi', 'wiki_code': 'pnb', 'lang_family': 'Indo-European'}, 'pol': {'lang': 'Polish', 'wiki_code': 'pl', 'lang_family': 'Indo-European'}, 'por': {'lang': 'Portuguese', 'wiki_code': 'pt', 'lang_family': 'Indo-European'}, 'pus': {'lang': 'Pushto', 'wiki_code': 'ps', 'lang_family': 'Indo-European'}, 'que': {'lang': 'Quechua', 'wiki_code': 'qu', 'lang_family': 'Quechuan'}, 'roa-tara': {'lang': 'Tarantino dialect', 'wiki_code': 'roa-tara', 'lang_family': 'Indo-European'}, 'roh': {'lang': 'Romansh', 'wiki_code': 'rm', 'lang_family': 'Indo-European'}, 'ron': {'lang': 'Romanian', 'wiki_code': 'ro', 'lang_family': 'Indo-European'}, 'rue': {'lang': 'Rusyn', 'wiki_code': 'rue', 'lang_family': 'Indo-European'}, 'rup': {'lang': 'Aromanian', 'wiki_code': 'roa-rup', 'lang_family': 'Indo-European'}, 'rus': {'lang': 'Russian', 'wiki_code': 'ru', 'lang_family': 'Indo-European'}, 'sah': {'lang': 'Yakut', 'wiki_code': 'sah', 'lang_family': 'Turkic'}, 'san': {'lang': 'Sanskrit', 'wiki_code': 'sa', 'lang_family': 'Indo-European'}, 'scn': {'lang': 'Sicilian', 'wiki_code': 'scn', 'lang_family': 'Indo-European'}, 'sco': {'lang': 'Scots', 'wiki_code': 'sco', 'lang_family': 'Indo-European'}, 'sgs': {'lang': 'Samogitian', 'wiki_code': 'bat-smg', 'lang_family': 'Indo-European'}, 'sin': {'lang': 'Sinhala', 'wiki_code': 'si', 'lang_family': 'Indo-European'}, 'slk': {'lang': 'Slovak', 'wiki_code': 'sk', 'lang_family': 'Indo-European'}, 'slv': {'lang': 'Slovene', 'wiki_code': 'sl', 'lang_family': 'Indo-European'}, 'sme': {'lang': 'Northern Sami', 'wiki_code': 'se', 'lang_family': 'Uralic'}, 'sna': {'lang': 'Shona', 'wiki_code': 'sn', 'lang_family': 'Niger-Congo'}, 'snd': {'lang': 'Sindhi', 'wiki_code': 'sd', 'lang_family': 'Indo-European'}, 'som': {'lang': 'Somali', 'wiki_code': 'so', 'lang_family': 'Afro-Asiatic'}, 'spa': {'lang': 'Spanish', 'wiki_code': 'es', 'lang_family': 'Indo-European'}, 'sqi': {'lang': 'Albanian', 'wiki_code': 'sq', 'lang_family': 'Indo-European'}, 'srd': {'lang': 'Sardinian', 'wiki_code': 'sc', 'lang_family': 'Indo-European'}, 'srn': {'lang': 'Sranan', 'wiki_code': 'srn', 'lang_family': 'Indo-European'}, 'srp': {'lang': 'Serbian', 'wiki_code': 'sr', 'lang_family': 'Indo-European'}, 'stq': {'lang': 'Saterfriesisch', 'wiki_code': 'stq', 'lang_family': 'Indo-European'}, 'sun': {'lang': 'Sundanese', 'wiki_code': 'su', 'lang_family': 'Austronesian'}, 'swa': {'lang': 'Swahili (macrolanguage)', 'wiki_code': 'sw', 'lang_family': 'Niger-Congo'}, 'swe': {'lang': 'Swedish', 'wiki_code': 'sv', 'lang_family': 'Indo-European'}, 'szl': {'lang': 'Silesian', 'wiki_code': 'szl', 'lang_family': 'Indo-European'}, 'tam': {'lang': 'Tamil', 'wiki_code': 'ta', 'lang_family': 'Dravidian'}, 'tat': {'lang': 'Tatar', 'wiki_code': 'tt', 'lang_family': 'Turkic'}, 'tcy': {'lang': 'Tulu', 'wiki_code': 'tcy', 'lang_family': 'Dravidian'}, 'tel': {'lang': 'Telugu', 'wiki_code': 'te', 'lang_family': 'Dravidian'}, 'tet': {'lang': 'Tetum', 'wiki_code': 'tet', 'lang_family': 'Austronesian'}, 'tgk': {'lang': 'Tajik', 'wiki_code': 'tg', 'lang_family': 'Indo-European'}, 'tgl': {'lang': 'Tagalog', 'wiki_code': 'tl', 'lang_family': 'Austronesian'}, 'tha': {'lang': 'Thai', 'wiki_code': 'th', 'lang_family': 'Tai-Kadai'}, 'ton': {'lang': 'Tongan', 'wiki_code': 'to', 'lang_family': 'Austronesian'}, 'tsn': {'lang': 'Tswana', 'wiki_code': 'tn', 'lang_family': 'Niger-Congo'}, 'tuk': {'lang': 'Turkmen', 'wiki_code': 'tk', 'lang_family': 'Turkic'}, 'tur': {'lang': 'Turkish', 'wiki_code': 'tr', 'lang_family': 'Turkic'}, 'tyv': {'lang': 'Tuvan', 'wiki_code': 'tyv', 'lang_family': 'Turkic'}, 'udm': {'lang': 'Udmurt', 'wiki_code': 'udm', 'lang_family': 'Uralic'}, 'uig': {'lang': 'Uighur', 'wiki_code': 'ug', 'lang_family': 'Turkic'}, 'ukr': {'lang': 'Ukrainian', 'wiki_code': 'uk', 'lang_family': 'Indo-European'}, 'urd': {'lang': 'Urdu', 'wiki_code': 'ur', 'lang_family': 'Indo-European'}, 'uzb': {'lang': 'Uzbek', 'wiki_code': 'uz', 'lang_family': 'Turkic'}, 'vec': {'lang': 'Venetian', 'wiki_code': 'vec', 'lang_family': 'Indo-European'}, 'vep': {'lang': 'Veps', 'wiki_code': 'vep', 'lang_family': 'Uralic'}, 'vie': {'lang': 'Vietnamese', 'wiki_code': 'vi', 'lang_family': 'Austronesian'}, 'vls': {'lang': 'Vlaams', 'wiki_code': 'vls', 'lang_family': 'Indo-European'}, 'vol': {'lang': 'Volapük', 'wiki_code': 'vo', 'lang_family': 'Constructed'}, 'vro': {'lang': 'Võro', 'wiki_code': 'fiu-vro', 'lang_family': 'Uralic'}, 'war': {'lang': 'Waray', 'wiki_code': 'war', 'lang_family': 'Austronesian'}, 'wln': {'lang': 'Walloon', 'wiki_code': 'wa', 'lang_family': 'Indo-European'}, 'wol': {'lang': 'Wolof', 'wiki_code': 'wo', 'lang_family': 'Niger-Congo'}, 'wuu': {'lang': 'Wu Chinese', 'wiki_code': 'wuu', 'lang_family': 'Sino-Tibetan'}, 'xho': {'lang': 'Xhosa', 'wiki_code': 'xh', 'lang_family': 'Niger-Congo'}, 'xmf': {'lang': 'Mingrelian', 'wiki_code': 'xmf', 'lang_family': 'Kartvelian'}, 'yid': {'lang': 'Yiddish', 'wiki_code': 'yi', 'lang_family': 'Indo-European'}, 'yor': {'lang': 'Yoruba', 'wiki_code': 'yo', 'lang_family': 'Niger-Congo'}, 'zea': {'lang': 'Zeeuws', 'wiki_code': 'zea', 'lang_family': 'Indo-European'}, 'zh-yue': {'lang': 'Cantonese', 'wiki_code': 'zh-yue', 'lang_family': 'Sino-Tibetan'}, 'zho': {'lang': 'Standard Chinese', 'wiki_code': 'zh', 'lang_family': 'Sino-Tibetan'}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hS6g_PetW9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7293a5f5-b177-4187-f70a-1ccf86db3cab"
      },
      "source": [
        "# dict that maps name to lang code\n",
        "name_to_label = {}\n",
        "for label in labels_dict:\n",
        "  name = labels_dict[label]['lang']\n",
        "  name_to_label[name] = label\n",
        "\n",
        "# example\n",
        "name_to_label['Udmurt']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'udm'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kk5AoDqhtXeK",
        "outputId": "546e55a8-d9d9-4152-f7b0-9583f99ddcab"
      },
      "source": [
        "# save train data into two lists, labels and articles\n",
        "x_train_path = '/content/gdrive/My Drive/wili-2018/x_train.txt'\n",
        "y_train_path = '/content/gdrive/My Drive/wili-2018/y_train.txt'\n",
        "\n",
        "with open(x_train_path, 'r') as fp:\n",
        "  x_train_list = fp.read().split('\\n')\n",
        "\n",
        "with open(y_train_path, 'r') as fp:\n",
        "  y_train_list = fp.read().split('\\n')\n",
        "\n",
        "\n",
        "# save test data into two lists, labels and articles\n",
        "x_test_path = '/content/gdrive/My Drive/wili-2018/x_test.txt'\n",
        "y_test_path = '/content/gdrive/My Drive/wili-2018/y_test.txt'\n",
        "\n",
        "with open(x_test_path, 'r') as fp:\n",
        "  x_test_list = fp.read().split('\\n')\n",
        "\n",
        "with open(y_test_path, 'r') as fp:\n",
        "  y_test_list = fp.read().split('\\n')\n",
        "\n",
        "# NOTE: test and train data are split 50/50 originally\n",
        "print(len(x_test_list))\n",
        "print(len(x_train_list))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "117501\n",
            "117501\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFj8809mni84",
        "outputId": "8a6cae1f-f9ad-48bb-f55d-fc85d9650c29"
      },
      "source": [
        "# combine provided train/test into single list\n",
        "x_list = x_train_list + x_test_list\n",
        "y_list = y_train_list + y_test_list\n",
        "\n",
        "print(len(x_list))\n",
        "print(len(y_list))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "235002\n",
            "235002\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ss2D_aIt-Bdj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24719a71-f586-47cf-fe36-d48e4d8ca8ac"
      },
      "source": [
        "# combine x,y lists into list of tuples: (label, text)\n",
        "list_double = [ (label, text) for text, label in zip(x_list, y_list)]\n",
        "\n",
        "# dict of languages to list of articles for each respective lang\n",
        "lang_dict = {}\n",
        "for key in labels_dict:\n",
        "   article_list = []\n",
        "   for label, text in list_double:\n",
        "     if key == label:\n",
        "       article_list.append(text)\n",
        "   lang_dict[key] = article_list\n",
        "\n",
        "# example\n",
        "lang_dict['ukr'][:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"Він веде успішну кар'єру вокаліста: бере участь у концертах та збірниках, і випускає сольні альбоми та сингли. Крім того, він виконує теми в «Анжеліку» OVA-3, «Таємничої грі: Легенда Вічного Світла» OVA-3 і «Білому Хресті»\",\n",
              " 'Протягом цього часу авторство багатьох композиції в репертуарі групи приписувалося Nanker Phelge — що означало, що вона є плодом спільних зусиль Джаггера-Джонса-Річардса-Уоттса-Вайман.',\n",
              " 'Про затвердження переліку автомобільних доріг загального користування державного значення: Кабінет Міністрів України; Постанова, Перелік від 16.09.2015 № 712',\n",
              " 'Рональд Рейган — єдиний президент Сполучених Штатів, який має власну зірку (за роль у фільмі Ковбой з Брукліна), а також один з двох губернаторів Каліфорнії (другим став Арнольд Шварценеггер).',\n",
              " 'Артемов А. А Памятники градостроительства и архитектуры Украинской ССР. Иллюстрированный справочник-каталог. В 4-х т. Т.2. Винницкая, Волынская, Ворошиловградская, Днепропетровская, Донецкая, Житомирская, Закарпатская, запорожская, Ивано-Франковская, Кировоградская, Крымская области. — Киев.: Будівельник, 1984. — 336с.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlUfpFMLQotJ"
      },
      "source": [
        "### 2) FastText Pretrained Model - Baseline Comparision"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cafyk4dBQ7yR"
      },
      "source": [
        "# langs to include in data since 235 is too many\n",
        "# top 10 langs minus Mandarin and Japanese (for sake of tokenization)\n",
        "# English, Spanish, Russian, Bengali, Portuquese, Punjabi, Hindi\n",
        "lang_list = ['eng', 'spa', 'rus', 'ben', 'por', 'pan', 'hin']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFrJZ7tdQn86"
      },
      "source": [
        "# get data only for languages in list\n",
        "x = []\n",
        "y = []\n",
        "for text, label in zip(x_list, y_list):\n",
        "  if label in lang_list:\n",
        "    x.append(text)\n",
        "    y.append(label)\n",
        "\n",
        "# split into train and test data - 80/20\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_list, y_list, test_size = 0.2, stratify = y_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FArA4IcNRFZ9"
      },
      "source": [
        "# build train data.txt in label text format\n",
        "output_file_1 = open('baseline_train_data.txt', 'w')\n",
        "for text, label in zip(x_train, y_train):\n",
        "  line = '__label__' + label + ' ' + text\n",
        "  output_file_1.write(line + '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKxc9H3wRJDw"
      },
      "source": [
        "# build test data.txt\n",
        "output_file_2 = open('baseline_test_data.txt', 'w')\n",
        "for text, label in zip(x_test, y_test):\n",
        "  line = '__label__' + label + ' ' + text\n",
        "  output_file_2.write(line + '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fd3DxyUNRO4j"
      },
      "source": [
        "# model_pretrained = fasttext.train_supervised('baseline_train_data.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knQxL86qR4Gi"
      },
      "source": [
        "# print(model_pretrained.test('baseline_test_data.txt'))\n",
        "# (46998, 0.8701859653602281, 0.8701859653602281)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NP3QIYGstggj"
      },
      "source": [
        "### 2) FastText with 10 Most Common Unigrams + Bag-of-Words\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DiMT6KZtgCe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04f3fbe5-1eb9-4592-8a79-08633419f1ee"
      },
      "source": [
        "import fasttext\n",
        "from sklearn.model_selection import train_test_split\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLvFR3DX-xKp"
      },
      "source": [
        "# langs to include in data since 235 is too many\n",
        "# top 10 langs minus Mandarin and Japanese (for sake of tokenization)\n",
        "# English, Spanish, Russian, Bengali, Portuquese, Punjabi, Hindi\n",
        "lang_list = ['eng', 'spa', 'rus', 'ben', 'por', 'pan', 'hin']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Hi3a6-Dtw6Y"
      },
      "source": [
        "# get data only for languages in list\n",
        "x = []\n",
        "y = []\n",
        "for text, label in zip(x_list, y_list):\n",
        "  if label in lang_list:\n",
        "    x.append(text)\n",
        "    y.append(label)\n",
        "\n",
        "# resplit into desired breakdown - 80/20\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, stratify = y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgY3iSgf6Ogp"
      },
      "source": [
        "# tokenize given doc - lowercase, discard of nums and punctuation, discard long tokens\n",
        "def tokenize(document):\n",
        "  tokens = nltk.word_tokenize(document)\n",
        "  words = [tok.lower() for tok in tokens if (tok.isalpha()) and (len(tok) < 10)]\n",
        "  return words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Todl4TWqDt6B"
      },
      "source": [
        "# build features based on x_train data\n",
        "# iterate thru langs, find 10 most common tokens for each\n",
        "word_features = []\n",
        "for lang in lang_list:\n",
        "  articles = []\n",
        "  for label, text in zip(y_train, x_train):\n",
        "    if label == lang:\n",
        "      articles.append(text)\n",
        "  document = ' '.join(articles)\n",
        "  tokens = tokenize(document)\n",
        "\n",
        "  lang_fd = nltk.FreqDist(tokens)\n",
        "  most_common = lang_fd.most_common(10)\n",
        "\n",
        "  for pair in most_common:\n",
        "    word = pair[0]\n",
        "    word_features.append(word)\n",
        "\n",
        "# remove duplicate tokens from feature list\n",
        "word_features = sorted(list(set(word_features)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Or3Xp8ivIX2s"
      },
      "source": [
        "# get train text file\n",
        "output_file_1 = open('train_data.txt', 'w')\n",
        "for text, label in zip(x_train, y_train):\n",
        "  # tokenize text\n",
        "  tokens = tokenize(text)\n",
        "\n",
        "  # iterate through feature words\n",
        "  # get num occurrences and add\n",
        "  feature_list = []\n",
        "  for num, word in enumerate(word_features):\n",
        "    count = tokens.count(word)\n",
        "    feature_list.append('ftr' + str(num) + ':' + str(count))\n",
        "\n",
        "  # get complete line, write to file\n",
        "  feature_line = ' '.join(feature_list)\n",
        "  lable_line = '__label__' + label \n",
        "  line = lable_line + ' ' + feature_line\n",
        "\n",
        "  output_file_1.write(line + '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzhtDeVfNOfF"
      },
      "source": [
        "# get test text file\n",
        "output_file_2 = open('test_data.txt', 'w')\n",
        "for text, label in zip(x_train, y_train):\n",
        "  # tokenize text\n",
        "  tokens = tokenize(text)\n",
        "  \n",
        "  # iterate through feature words\n",
        "  # get num occurrences and add\n",
        "  feature_list = []\n",
        "  for num, word in enumerate(word_features):\n",
        "    count = tokens.count(word)\n",
        "    feature_list.append('ftr' + str(num) + ':' + str(count))\n",
        "\n",
        "  # get complete line, write to file\n",
        "  feature_line = ' '.join(feature_list)\n",
        "  lable_line = '__label__' + label \n",
        "  line = lable_line + ' ' + feature_line\n",
        "\n",
        "  output_file_2.write(line + '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynknAxlMN6uz"
      },
      "source": [
        "# model_bow = fasttext.train_supervised('train_data.txt', epoch=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gs0G-WwEOMG8"
      },
      "source": [
        "# answer = model_bow.test('test_data.txt')\n",
        "# print(answer)\n",
        "# (5593, 0.9429644198104774, 0.9429644198104774)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yluoZfvsaAx"
      },
      "source": [
        "### 3) Putting everything together: 3 Resulting Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSoM8fr-GbjC"
      },
      "source": [
        "\"\"\"\n",
        "Get accuracy using given set of languages\n",
        "Using pretrained model\n",
        "params:\n",
        "lang_list     - languages to include in model\n",
        "x_list        - full x data from Wikipedia\n",
        "y_list        - full y data from Wikipedia\n",
        "\"\"\"\n",
        "def accuracy_for_pretrained(lang_list, x_list, y_list):\n",
        "  # get data only for languages in list\n",
        "  x = []\n",
        "  y = []\n",
        "  for text, label in zip(x_list, y_list):\n",
        "    if label in lang_list:\n",
        "      x.append(text)\n",
        "      y.append(label)\n",
        "\n",
        "  # split into train and test data - 80/20\n",
        "  x_train, x_test, y_train, y_test = train_test_split(x_list, y_list, test_size = 0.2, stratify = y_list)\n",
        "\n",
        "  # build train data.txt in label text format\n",
        "  output_file_1 = open('baseline_train_data.txt', 'w')\n",
        "  for text, label in zip(x_train, y_train):\n",
        "    line = '__label__' + label + ' ' + text\n",
        "    output_file_1.write(line + '\\n')\n",
        "\n",
        "  # build test set\n",
        "  output_file_2 = open('baseline_test_data.txt', 'w')\n",
        "  for text, label in zip(x_test, y_test):\n",
        "    line = '__label__' + label + ' ' + text\n",
        "    output_file_2.write(line + '\\n')\n",
        "\n",
        "  model = fasttext.train_supervised('baseline_train_data.txt')\n",
        "  answer = model.test('baseline_test_data.txt')\n",
        "\n",
        "  return answer[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfI9sRh9hvZx"
      },
      "source": [
        "\"\"\"\n",
        "Get accuracy using given set of languages\n",
        "Features: most common words\n",
        "params:\n",
        "lang_list     - languages to include in model\n",
        "num_common    - num most common words in features\n",
        "x_list        - full x data from Wikipedia\n",
        "y_list        - full y data from Wikipedia\n",
        "\"\"\"\n",
        "def accuracy_for_langs(lang_list, num_common, x_list, y_list):\n",
        "  x = []\n",
        "  y = []\n",
        "  for text, label in zip(x_list, y_list):\n",
        "    if label in lang_list:\n",
        "      x.append(text)\n",
        "      y.append(label)\n",
        "\n",
        "  # resplit into desired breakdown\n",
        "  x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, stratify = y)\n",
        "  \n",
        "  # get most common words in train set\n",
        "  word_features = []\n",
        "  for lang in lang_list:\n",
        "    articles = []\n",
        "    for label, text in zip(y_train, x_train):\n",
        "      if label == lang:\n",
        "        articles.append(text)\n",
        "    document = ' '.join(articles)\n",
        "    \n",
        "      tokens = nltk.word_tokenize(document)\n",
        "      tokens = [tok.lower() for tok in tokens if (tok.isalpha()) and (len(tok) < 10)]\n",
        "\n",
        "    lang_fd = nltk.FreqDist(tokens)\n",
        "    most_common = lang_fd.most_common(num_common)\n",
        "\n",
        "    for pair in most_common:\n",
        "      word = pair[0]\n",
        "      word_features.append(word)\n",
        "\n",
        "  # sort features and remove all duplicates\n",
        "  word_features = sorted(list(set(word_features)))\n",
        "  \n",
        "  # get train text file\n",
        "  output_file_1 = open('train_data.txt', 'w')\n",
        "  for text, label in zip(x_train, y_train):\n",
        "    # tokenize text\n",
        "    tokens = tokenize(text)\n",
        "\n",
        "    # iterate through feature words\n",
        "    # get num occurrences and add\n",
        "    feature_list = []\n",
        "    for num, word in enumerate(word_features):\n",
        "      count = tokens.count(word)\n",
        "      feature_list.append('ftr' + str(num) + ':' + str(count))\n",
        "\n",
        "    # get complete line, write to file\n",
        "    feature_line = ' '.join(feature_list)\n",
        "    lable_line = '__label__' + label \n",
        "    line = lable_line + ' ' + feature_line\n",
        "\n",
        "    output_file_1.write(line + '\\n')\n",
        "\n",
        "  # get test text file\n",
        "  output_file_2 = open('test_data.txt', 'w')\n",
        "  for text, label in zip(x_train, y_train):\n",
        "    # tokenize text\n",
        "    tokens = tokenize(text)\n",
        "    \n",
        "    # iterate through feature words\n",
        "    # get num occurrences and add\n",
        "    feature_list = []\n",
        "    for num, word in enumerate(word_features):\n",
        "      count = tokens.count(word)\n",
        "      feature_list.append('ftr' + str(num) + ':' + str(count))\n",
        "\n",
        "    # get complete line, write to file\n",
        "    feature_line = ' '.join(feature_list)\n",
        "    lable_line = '__label__' + label \n",
        "    line = lable_line + ' ' + feature_line\n",
        "\n",
        "    output_file_2.write(line + '\\n')\n",
        "\n",
        "  # train model and get accurracy for test data\n",
        "  model = fasttext.train_supervised('train_data.txt', epoch=20)\n",
        "  answer = model.test('test_data.txt')\n",
        "\n",
        "  return answer[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oxnl557uvF7x"
      },
      "source": [
        "\"\"\"\n",
        "Get accuracy using given set of languages\n",
        "Features: most common words + endings\n",
        "params:\n",
        "lang_list     - languages to include in model\n",
        "num_common    - num most common words in features\n",
        "x_list        - full x data from Wikipedia\n",
        "y_list        - full y data from Wikipedia\n",
        "\"\"\"\n",
        "def better_accuracy_for_langs(lang_list, num_common, x_list, y_list):\n",
        "  x = []\n",
        "  y = []\n",
        "  for text, label in zip(x_list, y_list):\n",
        "    if label in lang_list:\n",
        "      x.append(text)\n",
        "      y.append(label)\n",
        "\n",
        "  # resplit into desired breakdown\n",
        "  x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, stratify = y)\n",
        "  \n",
        "  # get most common words in train set\n",
        "  word_features = []\n",
        "  ending_features = []\n",
        "  for lang in lang_list:\n",
        "    articles = []\n",
        "    for label, text in zip(y_train, x_train):\n",
        "      if label == lang:\n",
        "        articles.append(text)\n",
        "    document = ' '.join(articles)\n",
        "    \n",
        "    # tokenize\n",
        "    tokens = nltk.word_tokenize(document)\n",
        "    tokens = [tok.lower() for tok in tokens if (tok.isalpha())]\n",
        "\n",
        "    # get most common words\n",
        "    short_tokens = [tok for tok in tokens if len(tok)<10]\n",
        "    lang_fd = nltk.FreqDist(short_tokens)\n",
        "    most_common_words = lang_fd.most_common(10)\n",
        "\n",
        "    for pair in most_common_words:\n",
        "      word = pair[0]\n",
        "      word_features.append(word)\n",
        "\n",
        "    # get most common endings\n",
        "    endings = [tok[-4:] for tok in tokens if len(tok)>4]\n",
        "    endings = [tok[-3:] for tok in tokens if len(tok)>3]\n",
        "    endings = [tok[-2:] for tok in tokens if len(tok)>2]\n",
        "    end_fd = nltk.FreqDist(endings)\n",
        "    most_common_endings = end_fd.most_common(5)\n",
        "\n",
        "    for pair in most_common_endings:\n",
        "      word = pair[0]\n",
        "      ending_features.append(word)\n",
        "\n",
        "  # sort features and remove all duplicates\n",
        "  features = sorted(list(set(word_features + ending_features)))\n",
        "\n",
        "  # get train text file\n",
        "  output_file_1 = open('train_data.txt', 'w')\n",
        "  for text, label in zip(x_train, y_train):\n",
        "    # tokenize text\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    tokens = [tok.lower() for tok in tokens if (tok.isalpha()) and (len(tok)<10)]\n",
        "\n",
        "    endings = [tok[-4:] for tok in tokens if len(tok)>4]\n",
        "    endings = [tok[-3:] for tok in tokens if len(tok)>3]\n",
        "    endings = [tok[-2:] for tok in tokens if len(tok)>2]\n",
        "\n",
        "    tokens = tokens + endings\n",
        "\n",
        "    # iterate through feature words\n",
        "    # get num occurrences and add\n",
        "    feature_list = []\n",
        "    for num, word in enumerate(features):\n",
        "      count = tokens.count(word)\n",
        "      feature_list.append('ftr' + str(num) + ':' + str(count))\n",
        "\n",
        "\n",
        "    # get complete line, write to file\n",
        "    feature_line = ' '.join(feature_list)\n",
        "    lable_line = '__label__' + label \n",
        "    line = lable_line + ' ' + feature_line\n",
        "\n",
        "    output_file_1.write(line + '\\n')\n",
        "\n",
        "  # get test text file\n",
        "  output_file_2 = open('test_data.txt', 'w')\n",
        "  for text, label in zip(x_train, y_train):\n",
        "    # tokenize text\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    tokens = [tok.lower() for tok in tokens if (tok.isalpha()) and (len(tok)<10)]\n",
        "\n",
        "    endings = [tok[-4:] for tok in tokens if len(tok)>4]\n",
        "    endings = [tok[-3:] for tok in tokens if len(tok)>3]\n",
        "    endings = [tok[-2:] for tok in tokens if len(tok)>2]\n",
        "\n",
        "    tokens = tokens + endings\n",
        "\n",
        "    # iterate through feature words\n",
        "    # get num occurrences and add\n",
        "    feature_list = []\n",
        "    for num, word in enumerate(features):\n",
        "      count = tokens.count(word)\n",
        "      feature_list.append('ftr' + str(num) + ':' + str(count))\n",
        "\n",
        "    # get complete line, write to file\n",
        "    feature_line = ' '.join(feature_list)\n",
        "    lable_line = '__label__' + label \n",
        "    line = lable_line + ' ' + feature_line\n",
        "\n",
        "    output_file_2.write(line + '\\n')\n",
        "\n",
        "  # train model and get accurracy for test data\n",
        "  model = fasttext.train_supervised('train_data.txt', epoch=20)\n",
        "  answer = model.test('test_data.txt')\n",
        "\n",
        "  return answer[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vc-qFRYA_SGj"
      },
      "source": [
        "### 4) Let's look at the results for different language groups and families:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G46hI5oPs1Fz"
      },
      "source": [
        "# Slavic: Russian, Ukrainian, Polish, Bulgarian, Serbian, Slovak, Macedonian, Croatian, Czech\n",
        "slavic_langs = ['rus','ukr','pol','bul','bel','srp','slk','mkd','slv','hrv','ces']\n",
        "\n",
        "# accuracy_for_langs(slavic_langs,10,x_list,y_list)\n",
        "# 0.9082422586520947\n",
        "\n",
        "# better_accuracy_for_langs(slavic_langs,10,x_list,y_list)\n",
        "# 0.9518181818181818\n",
        "\n",
        "# accuracy_for_pretrained(slavic_langs,x_list,y_list)\n",
        "# 0.8696642124526536"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xliiSpb5s9Eu"
      },
      "source": [
        "# Romance: Spanish, Portuguese, French, Romanian, Italian, Catalan, Galician, Lombard, Sardinian\n",
        "romance_langs = ['spa','por','fra','ron','ita','cat','glg','lmo','srd']\n",
        "\n",
        "# accuracy_for_langs(romance_langs,10,x_list,y_list)\n",
        "# 0.9388379204892966\n",
        "\n",
        "# better_accuracy_for_langs(romance_langs,10,x_list,y_list)\n",
        "# 0.9567514949242109\n",
        "\n",
        "#accuracy_for_pretrained(romance_langs,x_list,y_list)\n",
        "# 0.8684283313120997"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02ZLXitNtI5I"
      },
      "source": [
        "# Indic: Bengali, Oriya, Bhojpuri, Maithili, Sinhala, Gujarati, Hindi, Urdu, Panjabi, Sindhi\n",
        "indic_langs = ['ben','ori','bho','mai','sin','guj','hin','urd','pan','snd']\n",
        "\n",
        "# accuracy_for_langs(indic_langs,10,x_list,y_list)\n",
        "# 0.8380880880880881\n",
        "\n",
        "# better_accuracy_for_langs(indic_langs,10,x_list,y_list)\n",
        "# 0.8533533533533534\n",
        "\n",
        "# accuracy_for_pretrained(indic_langs,x_list,y_list)\n",
        "# 0.8672001362049885"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2o3z4kCFzSIY"
      },
      "source": [
        "# Germanic: English, German, Bavarian, Low German, Swedish, Danish, Afrikaans, Norwegian\n",
        "germanic_langs = ['eng','deu','bar','nds','swe','dan','afr','nob']\n",
        "\n",
        "# accuracy_for_langs(germanic_langs,10,x_list,y_list)\n",
        "# 0.94078125\n",
        "\n",
        "# better_accuracy_for_langs(germanic_langs,10,x_list,y_list)\n",
        "# 0.945244055068836\n",
        "\n",
        "# accuracy_for_pretrained(germanic_langs,x_list,y_list)\n",
        "# 0.8701091466139018"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRItbJ0867SQ"
      },
      "source": [
        "# Uralic: Finnish, Estonian, Hungarian, Komi, Udmurt\n",
        "uralic_langs = ['fin','est','hun','kom','udm']\n",
        "# accuracy_for_langs(uralic_langs,10,x_list,y_list)\n",
        "# 0.9121044701155199\n",
        "\n",
        "# better_accuracy_for_langs(uralic_langs,10,x_list,y_list)\n",
        "# 0.966\n",
        "\n",
        "# accuracy_for_pretrained(uralic_langs,x_list,y_list)\n",
        "# 0.8685464992551607"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdsidSx6G0dm"
      },
      "source": [
        "# Turkic: Turkish, Uzbek, Azerbaijani, Kazakh, Turkmen, Tatar, Kyrgyz, Bashkir, Chuvash, Karakalpak, Crimean Tatar, Tuvan\n",
        "turkic_langs = ['tur','uzb','aze','kaz','tuk','tat','kir','bak','chv','kaa','crh','sah','tuv']\n",
        "\n",
        "# accuracy_for_langs(turkic_langs,10,x_list,y_list)\n",
        "# 0.7941666666666667\n",
        "\n",
        "# better_accuracy_for_langs(turkic_langs,10,x_list,y_list)\n",
        "# 0.8903125\n",
        "\n",
        "# accuracy_for_pretrained(turkic_langs,x_list,y_list)\n",
        "# 0.8680100434079496"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00y2ljRBLntz"
      },
      "source": [
        "# Afroasiatic: Arabic, Hausa, Oromo, Amharic, Solami, Kabyle, Afar\n",
        "afroasiatic_langs = ['ara','hau','orm','amh','som','kab','aar']\n",
        "\n",
        "# accuracy_for_langs(afroasiatic_langs,10,x_list,y_list)\n",
        "# 0.9672096908939014\n",
        "\n",
        "# better_accuracy_for_langs(afroasiatic_langs,10,x_list,y_list)\n",
        "# 0.9774859287054409\n",
        "\n",
        "# accuracy_for_pretrained(afroasiatic_langs,x_list,y_list)\n",
        "# 0.8681533812827169"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUzt-0WCNZJf"
      },
      "source": [
        "### 5) Now, let's combine them and see how we do:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGh-VMqqQLd4"
      },
      "source": [
        "west_european_list = romance_langs + germanic_langs\n",
        "\n",
        "# accuracy_for_langs(west_european_list,10,x_list,y_list)\n",
        "# 0.9284348337746396\n",
        "\n",
        "# better_accuracy_for_langs(west_european_list,10,x_list,y_list)\n",
        "# 0.9414662842856092\n",
        "\n",
        "# accuracy_for_pretrained(west_european_list,x_list,y_list)\n",
        "# 0.8694782534683803"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvMYngkmQPJj"
      },
      "source": [
        "east_west_european_list = romance_langs + germanic_langs + slavic_langs\n",
        "\n",
        "# accuracy_for_langs(east_west_european_list,10,x_list,y_list)\n",
        "# 0.8897767857142858\n",
        "\n",
        "# better_accuracy_for_langs(east_west_european_list,10,x_list,y_list)\n",
        "# 0.9031163496740781\n",
        "\n",
        "# accuracy_for_pretrained(east_west_european_list,x_list,y_list)\n",
        "# 0.8688325780302547"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBQSdbSWPITo"
      },
      "source": [
        "european_list = slavic_langs + romance_langs + germanic_langs + uralic_langs\n",
        "\n",
        "# accuracy_for_langs(european_list,10,x_list,y_list)\n",
        "# 0.8218181818181818\n",
        "\n",
        "# better_accuracy_for_langs(european_list,10,x_list,y_list)\n",
        "# 0.838939393939394\n",
        "\n",
        "# accuracy_for_pretrained(european_list,x_list,y_list)\n",
        "# 0.8719591358944344"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ehu1prpQYib"
      },
      "source": [
        "indo_european_list = slavic_langs + romance_langs + germanic_langs + indic_langs\n",
        "\n",
        "# accuracy_for_langs(indo_european_list,10,x_list,y_list)\n",
        "# 0.6882894736842106\n",
        "\n",
        "# better_accuracy_for_langs(indo_european_list,10,x_list,y_list)\n",
        "# 0.5898684210526316\n",
        "\n",
        "# accuracy_for_pretrained(indo_european_list,x_list,y_list)\n",
        "# 0.8698742365884281"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8yxMeCt3jey"
      },
      "source": [
        "### 6) Lastly, combine all the languages and evaluate performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJJ0jB2tNymd"
      },
      "source": [
        "super_list = slavic_langs + romance_langs + indic_langs + germanic_langs + uralic_langs + turkic_langs + afroasiatic_langs\n",
        "\n",
        "# accuracy_for_langs(super_list,10,x_list,y_list)\n",
        "# 0.0984671502930448\n",
        "\n",
        "# better_accuracy_for_langs(super_list,10,x_list,y_list)\n",
        "# 0.17260245901639346\n",
        "\n",
        "#accuracy_for_pretrained(super_list,x_list,y_list)\n",
        "# 0.869256145578376"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_h8bt6UgBlo"
      },
      "source": [
        "super_list = slavic_langs + romance_langs + indic_langs + germanic_langs + uralic_langs + turkic_langs + afroasiatic_langs\n",
        "\n",
        "#accuracy_for_langs(super_list,5,x_list,y_list)\n",
        "# 0.5948565573770492\n",
        "\n",
        "# better_accuracy_for_langs(super_list,5,x_list,y_list)\n",
        "# 0.1794467213114754"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAZ0wXg4PvP6"
      },
      "source": [
        "super_list = slavic_langs + romance_langs + indic_langs + germanic_langs + uralic_langs + turkic_langs + afroasiatic_langs\n",
        "\n",
        "# accuracy_for_langs(super_list,15,x_list,y_list)\n",
        "# 0.044221311475409836\n",
        "\n",
        "# better_accuracy_for_langs(super_list,15,x_list,y_list)\n",
        "# 0.16649590163934427"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdh2kEaEP1Qf"
      },
      "source": [
        "super_list = slavic_langs + romance_langs + indic_langs + germanic_langs + uralic_langs + turkic_langs + afroasiatic_langs\n",
        "\n",
        "#accuracy_for_langs(super_list,20,x_list,y_list)\n",
        "# 0.08336065573770492\n",
        "\n",
        "# better_accuracy_for_langs(super_list,20,x_list,y_list)\n",
        "# 0.17413934426229508"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4PJooJWP2h7"
      },
      "source": [
        "super_list = slavic_langs + romance_langs + indic_langs + germanic_langs + uralic_langs + turkic_langs + afroasiatic_langs\n",
        "\n",
        "#accuracy_for_langs(super_list,25,x_list,y_list)\n",
        "# 0.16362704918032786\n",
        "\n",
        "# better_accuracy_for_langs(super_list,25,x_list,y_list)\n",
        "# 0.17461065573770493"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5tjbmRwmRg8"
      },
      "source": [
        "super_list = slavic_langs + romance_langs + indic_langs + germanic_langs + uralic_langs + turkic_langs + afroasiatic_langs\n",
        "\n",
        "#accuracy_for_langs(super_list,30,x_list,y_list)\n",
        "# 0.043852459016393446\n",
        "\n",
        "# better_accuracy_for_langs(super_list,30,x_list,y_list)\n",
        "# 0.1737295081967213"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PiZGkn8BmSGg"
      },
      "source": [
        "super_list = slavic_langs + romance_langs + indic_langs + germanic_langs + uralic_langs + turkic_langs + afroasiatic_langs\n",
        "\n",
        "# accuracy_for_langs(super_list,35,x_list,y_list)\n",
        "# 0.07338114754098361\n",
        "\n",
        "#better_accuracy_for_langs(super_list,35,x_list,y_list)\n",
        "# 0.18"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}